{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcba82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from itertools import combinations, product\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import random\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67603306",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "MODEL = \"openai/gpt-4.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b556a",
   "metadata": {},
   "source": [
    "### Aggregate JSON results into Parquet and CSV\n",
    "\n",
    "โค้ดด้านล่างจะ:\n",
    "- ค้นหาโฟลเดอร์ `infomation/testing/<model>/results` อัตโนมัติ\n",
    "- อ่านไฟล์ `.json` ทั้งหมดรวมเป็น DataFrame ต่อโมเดล\n",
    "- บันทึก `all_results.csv` (เข้ารหัส UTF-8-SIG รองรับภาษาไทย) และ `all_results.parquet` ในโฟลเดอร์ของแต่ละโมเดล\n",
    "- แสดงสรุปจำนวนไฟล์/แถว/เส้นทางที่บันทึก\n",
    "\n",
    "หมายเหตุ:\n",
    "- สำหรับ Parquet แนะนำให้ติดตั้ง `pyarrow` (หรือ `fastparquet`) หากยังไม่ได้ติดตั้ง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03002e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing root: /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing\n",
      "Found model folders: ['thai-homeworkgen-v4', 'gemini-2.5-pro', 'chatgpt-4o-mini']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>json_files</th>\n",
       "      <th>rows</th>\n",
       "      <th>csv</th>\n",
       "      <th>parquet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thai-homeworkgen-v4</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>/Users/tk17250/Documents/Work/UpMath/UpMathLLM...</td>\n",
       "      <td>/Users/tk17250/Documents/Work/UpMath/UpMathLLM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>/Users/tk17250/Documents/Work/UpMath/UpMathLLM...</td>\n",
       "      <td>/Users/tk17250/Documents/Work/UpMath/UpMathLLM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chatgpt-4o-mini</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>/Users/tk17250/Documents/Work/UpMath/UpMathLLM...</td>\n",
       "      <td>/Users/tk17250/Documents/Work/UpMath/UpMathLLM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  json_files  rows  \\\n",
       "0  thai-homeworkgen-v4         156   156   \n",
       "1       gemini-2.5-pro         189   189   \n",
       "2      chatgpt-4o-mini         189   189   \n",
       "\n",
       "                                                 csv  \\\n",
       "0  /Users/tk17250/Documents/Work/UpMath/UpMathLLM...   \n",
       "1  /Users/tk17250/Documents/Work/UpMath/UpMathLLM...   \n",
       "2  /Users/tk17250/Documents/Work/UpMath/UpMathLLM...   \n",
       "\n",
       "                                             parquet  \n",
       "0  /Users/tk17250/Documents/Work/UpMath/UpMathLLM...  \n",
       "1  /Users/tk17250/Documents/Work/UpMath/UpMathLLM...  \n",
       "2  /Users/tk17250/Documents/Work/UpMath/UpMathLLM...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Root directory that contains model folders\n",
    "testing_root = Path(\".\").resolve() / \"infomation\" / \"testing\"\n",
    "\n",
    "# Fallback if __file__ is not defined (e.g., in notebook context)\n",
    "if not testing_root.exists():\n",
    "    nb_root = Path.cwd()\n",
    "    # Try to locate the repo root by finding 'infomation/testing'\n",
    "    candidate = nb_root / \"infomation\" / \"testing\"\n",
    "    if candidate.exists():\n",
    "        testing_root = candidate\n",
    "    else:\n",
    "        # Walk up a few levels\n",
    "        for _ in range(5):\n",
    "            nb_root = nb_root.parent\n",
    "            candidate = nb_root / \"infomation\" / \"testing\"\n",
    "            if candidate.exists():\n",
    "                testing_root = candidate\n",
    "                break\n",
    "\n",
    "print(f\"Testing root: {testing_root}\")\n",
    "assert testing_root.exists(), \"Cannot locate 'infomation/testing' directory. Please adjust the path.\"\n",
    "\n",
    "# Discover model directories (directories containing a 'results' subfolder)\n",
    "model_dirs = [p for p in testing_root.iterdir() if p.is_dir() and (p / \"results\").is_dir()]\n",
    "print(\"Found model folders:\", [p.name for p in model_dirs])\n",
    "\n",
    "summary: List[Dict[str, Any]] = []\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    model_name = model_dir.name\n",
    "    results_dir = model_dir / \"results\"\n",
    "    json_files = sorted(results_dir.glob(\"*.json\"))\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            # Normalize nested JSON into flat columns when possible\n",
    "            rec = {}\n",
    "            if isinstance(data, dict):\n",
    "                rec = data.copy()\n",
    "            else:\n",
    "                rec = {\"raw\": data}\n",
    "            rec[\"_source_file\"] = str(jf)\n",
    "            records.append(rec)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {jf}: {e}\")\n",
    "\n",
    "    if not records:\n",
    "        print(f\"No JSON files found for model '{model_name}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.json_normalize(records, sep=\".\")\n",
    "\n",
    "    # Write outputs\n",
    "    csv_path = model_dir / \"all_results.csv\"\n",
    "    parquet_path = model_dir / \"all_results.parquet\"\n",
    "\n",
    "    # CSV with UTF-8-SIG for Thai compatibility in Excel\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Parquet requires pyarrow or fastparquet\n",
    "    try:\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        parquet_ok = True\n",
    "    except Exception as e:\n",
    "        print(f\"Parquet write failed for '{model_name}': {e}\")\n",
    "        parquet_ok = False\n",
    "\n",
    "    summary.append({\n",
    "        \"model\": model_name,\n",
    "        \"json_files\": len(json_files),\n",
    "        \"rows\": len(df),\n",
    "        \"csv\": str(csv_path),\n",
    "        \"parquet\": str(parquet_path) if parquet_ok else None,\n",
    "    })\n",
    "\n",
    "# Display summary\n",
    "pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a77f5",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2441ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OPENROUTER_API_KEY = \"xxx\"\n",
    "EVAL_ENDPOINT = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "EVAL_MODEL = \"openai/gpt-4.1\"  # OpenRouter slug\n",
    "MODEL_EVALUATE_FIELD = \"gpt-4.1\"  # Value to record in output\n",
    "EVAL_CONCURRENCY = int(os.getenv(\"EVAL_CONCURRENCY\", \"4\"))  # parallel requests\n",
    "MAX_RETRIES = 2\n",
    "TIMEOUT = 120  # seconds per request\n",
    "\n",
    "assert OPENROUTER_API_KEY, \"Environment variable OPENROUTER_API_KEY is not set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7747490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing root: /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # Optional but recommended for OpenRouter policy\n",
    "    \"HTTP-Referer\": os.getenv(\"OPENROUTER_HTTP_REFERER\", \"http://localhost\"),\n",
    "    \"X-Title\": os.getenv(\"OPENROUTER_TITLE\", \"UpMathLLM Evaluation\"),\n",
    "}\n",
    "\n",
    "# Locate testing root containing model folders and all_results.* files\n",
    "testing_root = Path(\".\").resolve() / \"infomation\" / \"testing\"\n",
    "if not testing_root.exists():\n",
    "    nb_root = Path.cwd()\n",
    "    candidate = nb_root / \"infomation\" / \"testing\"\n",
    "    if candidate.exists():\n",
    "        testing_root = candidate\n",
    "    else:\n",
    "        for _ in range(5):\n",
    "            nb_root = nb_root.parent\n",
    "            candidate = nb_root / \"infomation\" / \"testing\"\n",
    "            if candidate.exists():\n",
    "                testing_root = candidate\n",
    "                break\n",
    "\n",
    "print(f\"Testing root: {testing_root}\")\n",
    "assert testing_root.exists(), \"Cannot locate 'infomation/testing' directory. Please adjust the path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16265165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to evaluate: ['thai-homeworkgen-v4', 'gemini-2.5-pro', 'chatgpt-4o-mini']\n"
     ]
    }
   ],
   "source": [
    "# Discover model folders\n",
    "model_dirs = [p for p in testing_root.iterdir() if p.is_dir()]\n",
    "model_dirs = [p for p in model_dirs if (p / \"all_results.csv\").exists() or (p / \"all_results.parquet\").exists()]\n",
    "print(\"Models to evaluate:\", [p.name for p in model_dirs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5401cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_keys = [\n",
    "    (\"score_katex_formatting\", \"KaTeX Formatting (0-10)\"),\n",
    "    (\"score_mathematical_correctness\", \"Mathematical Correctness (0-10)\"),\n",
    "    (\"score_explanation_clarity\", \"Explanation Clarity (0-10)\"),\n",
    "    (\"score_difficulty_alignment\", \"Difficulty Alignment (0-10)\"),\n",
    "    (\"score_blooms_taxonomy\", \"Bloom's Taxonomy (0-10)\"),\n",
    "    (\"score_thai_grammar_style\", \"Grammar & Style (0-10)\"),\n",
    "    (\"score_thai_math_terms\", \"Mathematical Thai Terms (0-10)\"),\n",
    "]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a meticulous Thai math education evaluator. Score the provided question+explanation content \"\n",
    "    \"across specific criteria. Return strict JSON only. Use Thai language in think/explanations. \"\n",
    "    \"Each score is an integer 0-10. If a criterion does not apply, still provide a best-effort score.\"\n",
    ")\n",
    "\n",
    "rubric_instructions = (\n",
    "    \"Rubric:\\n\"\n",
    "    \"1) Technical Accuracy\\n\"\n",
    "    \"- KaTeX Formatting (0-10): KaTeX/LaTeX syntax validity, renderability.\\n\"\n",
    "    \"- Mathematical Correctness (0-10): Correct results and steps.\\n\"\n",
    "    \"2) Content Quality\\n\"\n",
    "    \"- Explanation Clarity (0-10): Clear, step-by-step explanation.\\n\"\n",
    "    \"- Difficulty Alignment (0-10): Difficulty matches the described level.\\n\"\n",
    "    \"- Bloom's Taxonomy (0-10): Correct mapping of skills to Bloom's levels.\\n\"\n",
    "    \"3) Thai Language\\n\"\n",
    "    \"- Grammar & Style (0-10): Thai grammar and style quality.\\n\"\n",
    "    \"- Mathematical Thai Terms (0-10): Correct Thai math terminology.\\n\\n\"\n",
    "    \"Output JSON schema:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"scores\\\": {\\n\"\n",
    "    \"    \\\"katex_formatting\\\": <int 0-10>,\\n\"\n",
    "    \"    \\\"mathematical_correctness\\\": <int 0-10>,\\n\"\n",
    "    \"    \\\"explanation_clarity\\\": <int 0-10>,\\n\"\n",
    "    \"    \\\"difficulty_alignment\\\": <int 0-10>,\\n\"\n",
    "    \"    \\\"blooms_taxonomy\\\": <int 0-10>,\\n\"\n",
    "    \"    \\\"thai_grammar_style\\\": <int 0-10>,\\n\"\n",
    "    \"    \\\"thai_math_terms\\\": <int 0-10>\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  \\\"average_score\\\": <float>,\\n\"\n",
    "    \"  \\\"think\\\": <string in Thai summarizing reasoning>\\n\"\n",
    "    \"}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e07d35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_score(x: Any) -> Optional[int]:\n",
    "    try:\n",
    "        v = int(round(float(x)))\n",
    "        return max(0, min(10, v))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_average(scores: Dict[str, Any]) -> Optional[float]:\n",
    "    vals: List[float] = []\n",
    "    for k in [\n",
    "        \"katex_formatting\",\n",
    "        \"mathematical_correctness\",\n",
    "        \"explanation_clarity\",\n",
    "        \"difficulty_alignment\",\n",
    "        \"blooms_taxonomy\",\n",
    "        \"thai_grammar_style\",\n",
    "        \"thai_math_terms\",\n",
    "    ]:\n",
    "        v = scores.get(k)\n",
    "        if v is None:\n",
    "            continue\n",
    "        try:\n",
    "            vals.append(float(v))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not vals:\n",
    "        return None\n",
    "    return sum(vals) / len(vals)\n",
    "\n",
    "\n",
    "def call_openrouter_eval(content: str, meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    user_context = {\n",
    "        \"topic\": meta.get(\"topic\"),\n",
    "        \"grade\": meta.get(\"grade\"),\n",
    "        \"qtype\": meta.get(\"qtype\"),\n",
    "        \"level\": meta.get(\"level\"),\n",
    "        \"bloom\": meta.get(\"bloom\"),\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{rubric_instructions}\\n\\nContent to evaluate (Thai):\\n{content}\\n\\nContext: {json.dumps(user_context, ensure_ascii=False)}\"},\n",
    "    ]\n",
    "    payload = {\n",
    "        \"model\": EVAL_MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "    }\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            # small jitter to avoid bursts\n",
    "            time.sleep(random.uniform(0.05, 0.2))\n",
    "            resp = requests.post(EVAL_ENDPOINT, headers=headers, data=json.dumps(payload), timeout=TIMEOUT)\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                msg = data.get(\"choices\", [{}])[0].get(\"message\", {})\n",
    "                content_json = msg.get(\"content\", \"{}\")\n",
    "                # Parse returned JSON content\n",
    "                try:\n",
    "                    parsed = json.loads(content_json)\n",
    "                except Exception:\n",
    "                    # try to strip code fences if any\n",
    "                    cleaned = content_json.strip().strip('`')\n",
    "                    parsed = json.loads(cleaned)\n",
    "                return parsed\n",
    "            else:\n",
    "                last_err = f\"HTTP {resp.status_code}: {resp.text[:300]}\"\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "        # backoff\n",
    "        time.sleep(0.5 * (attempt + 1))\n",
    "    raise RuntimeError(last_err or \"Unknown evaluation error\")\n",
    "\n",
    "\n",
    "def evaluate_row(row: pd.Series, fallback_model_name: str) -> Dict[str, Any]:\n",
    "    item_id = row.get(\"id\")\n",
    "    content = row.get(\"content\") or \"\"\n",
    "    # Resolve model name\n",
    "    meta_model = row.get(\"meta.model\")\n",
    "    model_name = meta_model if (isinstance(meta_model, str) and meta_model.strip()) else fallback_model_name\n",
    "    # Special case capitalization for Thai-HomeworkGen-v4\n",
    "    if fallback_model_name.lower() == \"thai-homeworkgen-v4\":\n",
    "        model_name = \"Thai-HomeworkGen-v4\" if not (isinstance(meta_model, str) and meta_model.strip()) else meta_model\n",
    "\n",
    "    meta = {\n",
    "        \"topic\": row.get(\"topic\"),\n",
    "        \"grade\": row.get(\"grade\"),\n",
    "        \"qtype\": row.get(\"qtype\"),\n",
    "        \"level\": row.get(\"level\"),\n",
    "        \"bloom\": row.get(\"bloom\"),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = call_openrouter_eval(str(content), meta)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"id\": item_id,\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e),\n",
    "            \"model_evaluate\": MODEL_EVALUATE_FIELD,\n",
    "        }\n",
    "\n",
    "    scores = resp.get(\"scores\", {}) if isinstance(resp, dict) else {}\n",
    "    out = {\n",
    "        \"id\": item_id,\n",
    "        \"model\": model_name,\n",
    "        \"score_katex_formatting\": clamp_score(scores.get(\"katex_formatting\")),\n",
    "        \"score_mathematical_correctness\": clamp_score(scores.get(\"mathematical_correctness\")),\n",
    "        \"score_explanation_clarity\": clamp_score(scores.get(\"explanation_clarity\")),\n",
    "        \"score_difficulty_alignment\": clamp_score(scores.get(\"difficulty_alignment\")),\n",
    "        \"score_blooms_taxonomy\": clamp_score(scores.get(\"blooms_taxonomy\")),\n",
    "        \"score_thai_grammar_style\": clamp_score(scores.get(\"thai_grammar_style\")),\n",
    "        \"score_thai_math_terms\": clamp_score(scores.get(\"thai_math_terms\")),\n",
    "        \"average_score\": resp.get(\"average_score\"),\n",
    "        \"think\": resp.get(\"think\"),\n",
    "        \"model_evaluate\": MODEL_EVALUATE_FIELD,\n",
    "    }\n",
    "    # Compute average if missing\n",
    "    if out[\"average_score\"] is None or (isinstance(out[\"average_score\"], float) and math.isnan(out[\"average_score\"])):\n",
    "        avg = compute_average({\n",
    "            \"katex_formatting\": out[\"score_katex_formatting\"],\n",
    "            \"mathematical_correctness\": out[\"score_mathematical_correctness\"],\n",
    "            \"explanation_clarity\": out[\"score_explanation_clarity\"],\n",
    "            \"difficulty_alignment\": out[\"score_difficulty_alignment\"],\n",
    "            \"blooms_taxonomy\": out[\"score_blooms_taxonomy\"],\n",
    "            \"thai_grammar_style\": out[\"score_thai_grammar_style\"],\n",
    "            \"thai_math_terms\": out[\"score_thai_math_terms\"],\n",
    "        })\n",
    "        out[\"average_score\"] = avg\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "815dea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results: List[pd.DataFrame] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95afaf1",
   "metadata": {},
   "source": [
    "#### All model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "513f6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model folder: thai-homeworkgen-v4\n",
      "Already processed: 0 rows\n",
      "To evaluate now: 156 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing thai-homeworkgen-v4: 100%|██████████| 156/156 [03:01<00:00,  1.16s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 156 rows -> /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing/thai-homeworkgen-v4/evaluation_gpt-4.1.csv\n",
      "Saved/updated: /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing/thai-homeworkgen-v4/evaluation_gpt-4.1.csv (total 156 rows)\n",
      "\n",
      "Evaluating model folder: gemini-2.5-pro\n",
      "Already processed: 0 rows\n",
      "To evaluate now: 189 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing gemini-2.5-pro: 100%|██████████| 189/189 [03:01<00:00,  1.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 189 rows -> /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing/gemini-2.5-pro/evaluation_gpt-4.1.csv\n",
      "Saved/updated: /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing/gemini-2.5-pro/evaluation_gpt-4.1.csv (total 189 rows)\n",
      "\n",
      "Evaluating model folder: chatgpt-4o-mini\n",
      "Already processed: 0 rows\n",
      "To evaluate now: 189 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing chatgpt-4o-mini: 100%|██████████| 189/189 [03:12<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 189 rows -> /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing/chatgpt-4o-mini/evaluation_gpt-4.1.csv\n",
      "Saved/updated: /Users/tk17250/Documents/Work/UpMath/UpMathLLM/infomation/testing/chatgpt-4o-mini/evaluation_gpt-4.1.csv (total 189 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model_dir in model_dirs:\n",
    "    model_folder = model_dir.name\n",
    "    print(f\"\\nEvaluating model folder: {model_folder}\")\n",
    "    # Load aggregated results\n",
    "    df = None\n",
    "    csv_path = model_dir / \"all_results.csv\"\n",
    "    pq_path = model_dir / \"all_results.parquet\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "    elif pq_path.exists():\n",
    "        df = pd.read_parquet(pq_path)\n",
    "    else:\n",
    "        print(f\"No all_results.* found in {model_dir}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No rows to evaluate.\")\n",
    "        continue\n",
    "\n",
    "    # Output paths\n",
    "    out_csv = model_dir / f\"evaluation_{MODEL_EVALUATE_FIELD}.csv\"\n",
    "    out_parquet = model_dir / f\"evaluation_{MODEL_EVALUATE_FIELD}.parquet\"\n",
    "\n",
    "    # Resumability: skip rows already evaluated\n",
    "    processed_ids = set()\n",
    "    header_exists = out_csv.exists()\n",
    "    if header_exists:\n",
    "        try:\n",
    "            prev = pd.read_csv(out_csv, usecols=[\"id\"], dtype=str)\n",
    "            processed_ids = set(prev[\"id\"].astype(str).tolist())\n",
    "        except Exception:\n",
    "            # If legacy or empty, ignore\n",
    "            processed_ids = set()\n",
    "    print(f\"Already processed: {len(processed_ids)} rows\")\n",
    "\n",
    "    rows_to_eval = [r[1] for r in df.iterrows() if str(r[1].get(\"id\")) not in processed_ids]\n",
    "    if not rows_to_eval:\n",
    "        print(\"Nothing new to evaluate. Skipping to Parquet sync...\")\n",
    "        # Ensure parquet syncs with current CSV\n",
    "        try:\n",
    "            cur = pd.read_csv(out_csv)\n",
    "            cur.to_parquet(out_parquet, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Parquet sync failed for {model_folder}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"To evaluate now: {len(rows_to_eval)} rows\")\n",
    "\n",
    "    # Prepare CSV append writer\n",
    "    field_order = [\n",
    "        \"id\",\n",
    "        \"model\",\n",
    "        \"score_katex_formatting\",\n",
    "        \"score_mathematical_correctness\",\n",
    "        \"score_explanation_clarity\",\n",
    "        \"score_difficulty_alignment\",\n",
    "        \"score_blooms_taxonomy\",\n",
    "        \"score_thai_grammar_style\",\n",
    "        \"score_thai_math_terms\",\n",
    "        \"average_score\",\n",
    "        \"think\",\n",
    "        \"model_evaluate\",\n",
    "        \"error\",\n",
    "    ]\n",
    "\n",
    "    # Append results as they complete\n",
    "    writes = 0\n",
    "    with ThreadPoolExecutor(max_workers=EVAL_CONCURRENCY) as ex:\n",
    "        futures = [ex.submit(evaluate_row, row, model_folder) for row in rows_to_eval]\n",
    "        # Open file once for append; we’ll write header if creating new file\n",
    "        with open(out_csv, \"a\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=field_order)\n",
    "            if not header_exists:\n",
    "                writer.writeheader()\n",
    "                header_exists = True\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"Writing {model_folder}\"):\n",
    "                res = fut.result()\n",
    "                # Ensure all fields exist\n",
    "                for k in field_order:\n",
    "                    res.setdefault(k, None)\n",
    "                writer.writerow({k: res.get(k) for k in field_order})\n",
    "                writes += 1\n",
    "\n",
    "    print(f\"Appended {writes} rows -> {out_csv}\")\n",
    "\n",
    "    # Refresh Parquet from the full CSV after this batch\n",
    "    try:\n",
    "        eval_df = pd.read_csv(out_csv)\n",
    "        eval_df.to_parquet(out_parquet, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Parquet write failed for {model_folder}: {e}\")\n",
    "\n",
    "    # Track in-memory summary for combined\n",
    "    try:\n",
    "        all_results.append(eval_df.assign(_model_folder=model_folder))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Saved/updated: {out_csv} (total {len(eval_df)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f7ef85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined saved: /Users/tk17250/Documents/Work/UpMath/UpMathLLM/src/upmathllm/evaluation_gpt-4.1_all_models.csv (534 rows)\n"
     ]
    }
   ],
   "source": [
    "# Optional combined summary across all models\n",
    "if all_results:\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    combined_csv = Path(\".\").resolve() / f\"evaluation_{MODEL_EVALUATE_FIELD}_all_models.csv\"\n",
    "    combined_parquet = Path(\".\").resolve() / f\"evaluation_{MODEL_EVALUATE_FIELD}_all_models.parquet\"\n",
    "    combined.to_csv(combined_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    try:\n",
    "        combined.to_parquet(combined_parquet, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Combined Parquet write failed: {e}\")\n",
    "    print(f\"\\nCombined saved: {combined_csv} ({len(combined)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd47328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a small sample\n",
    "combined.head(10) if all_results else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a10661",
   "metadata": {},
   "source": [
    "#### Test once (one-shot evaluation)\n",
    "\n",
    "ใช้เซลล์ด้านล่างเพื่อทดสอบประเมิน 1 แถวแบบโต้ตอบ โดยไม่เขียนไฟล์ออก ปรับ index หรือกรองข้อมูลตามต้องการก่อนเรียก `evaluate_row`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d31f5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'sync-00966335-5305-46ac-9644-9c8846fdcd1b-e2',\n",
       " 'model': 'Thai-HomeworkGen-v4',\n",
       " 'score_katex_formatting': 10,\n",
       " 'score_mathematical_correctness': 10,\n",
       " 'score_explanation_clarity': 9,\n",
       " 'score_difficulty_alignment': 10,\n",
       " 'score_blooms_taxonomy': 8,\n",
       " 'score_thai_grammar_style': 9,\n",
       " 'score_thai_math_terms': 10,\n",
       " 'average_score': 9.43,\n",
       " 'think': \"คำถามและคำอธิบายมีการใช้ KaTeX ได้ถูกต้องสมบูรณ์ ไม่มีข้อผิดพลาดทางคณิตศาสตร์ อธิบายขั้นตอนอย่างชัดเจนและเป็นลำดับ เหมาะสมกับระดับ 'ง่าย' ของ ม.5 และสอดคล้องกับระดับ Bloom's 'วิเคราะห์' แม้จะเน้นการแยกตัวประกอบและตัดทอนซึ่งอาจอยู่ระหว่าง 'เข้าใจ' กับ 'วิเคราะห์' การใช้ภาษาไทยถูกต้องและใช้คำศัพท์คณิตศาสตร์ได้เหมาะสม มีจุดเล็กน้อยที่อาจเพิ่มความชัดเจนในคำอธิบาย เช่น การเน้นเหตุผลที่ $x \\\\neq 3$ แต่โดยรวมถือว่าดีมาก\",\n",
       " 'model_evaluate': 'gpt-4.1'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure test target\n",
    "TEST_MODEL_FOLDER = os.getenv(\"TEST_MODEL_FOLDER\", \"thai-homeworkgen-v4\")  # change to any model folder name\n",
    "TEST_ROW_INDEX = int(os.getenv(\"TEST_ROW_INDEX\", \"0\"))  # pick row index to evaluate\n",
    "\n",
    "# Load that model’s aggregated data\n",
    "test_model_dir = next((p for p in model_dirs if p.name == TEST_MODEL_FOLDER), None)\n",
    "assert test_model_dir is not None, f\"Model folder '{TEST_MODEL_FOLDER}' not found under {testing_root}\"\n",
    "\n",
    "csv_path = test_model_dir / \"all_results.csv\"\n",
    "pq_path = test_model_dir / \"all_results.parquet\"\n",
    "if csv_path.exists():\n",
    "    df_test = pd.read_csv(csv_path)\n",
    "elif pq_path.exists():\n",
    "    df_test = pd.read_parquet(pq_path)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No all_results.* found in {test_model_dir}\")\n",
    "\n",
    "assert len(df_test) > 0, \"No rows in aggregated data\"\n",
    "row = df_test.iloc[TEST_ROW_INDEX]\n",
    "res = evaluate_row(row, test_model_dir.name)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upmathllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
