{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z4GzaccJNZe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install vllm pandas tqdm joblib\n",
    "!pip install -q transformers accelerate einops safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA in torch:\", torch.version.cuda)  # ควรขึ้น 12.8\n",
    "print(\"Capability:\", torch.cuda.get_device_capability(0))  # ควรเป็น (12, 0)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1754941987731,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "_Y-y74kFJew-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import textwrap\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from itertools import combinations, product\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1754941988765,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "8ajomihUsPeR"
   },
   "outputs": [],
   "source": [
    "BLOOM_LEVELS = [\"จำ\", \"เข้าใจ\", \"นำไปใช้\", \"วิเคราะห์\", \"ประเมิน\", \"สร้างสรรค์\"]\n",
    "LEVELS = [\"ง่าย\", \"ปานกลาง\", \"ยาก\"]\n",
    "GRADE_LEVELS = [\"ม.4\", \"ม.5\", \"ม.6\"]\n",
    "TOPICS = [\"พีชคณิต\"]\n",
    "QUESTION_TYPES = [\"multiple_choice\"]\n",
    "\n",
    "N_JOBS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1754941989577,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "_7WFS2BNsVYg",
    "outputId": "0b4d11c0-f965-42ea-bd4e-c5c59481b32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 189\n"
     ]
    }
   ],
   "source": [
    "bloom_combinations = [[b] for b in BLOOM_LEVELS] + [list(pair) for pair in combinations(BLOOM_LEVELS, 2)]\n",
    "all_combinations = list(product(TOPICS, GRADE_LEVELS, QUESTION_TYPES, LEVELS, bloom_combinations))\n",
    "print(f\"Total combinations: {len(all_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1754942032960,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "573o8nVqsfec"
   },
   "outputs": [],
   "source": [
    "def create_user_prompt(topic, grade_level, question_type, difficulty, bloom_levels, num_problems=1):\n",
    "    bloom_str = \", \".join(bloom_levels)\n",
    "    prompt = f\"\"\"จงสร้างโจทย์คณิตศาสตร์คุณภาพสูงโดยกำหนดให้\n",
    "1. หัวข้อ: {topic}\n",
    "2. สำหรับนักเรียน: {grade_level}\n",
    "3. รูปแบบ: {question_type}\n",
    "4. ความยาก: {difficulty}\n",
    "5. bloom level: {bloom_str}\n",
    "6. จำนวน: {num_problems} ข้อ\n",
    "7. เพิ่มเติม: โจทย์จำเป็นต้องมีคำตอบ และถ้าโจทย์เป็นแบบ multiple choice (ปรนัย) ต้องมีคำตอบหลอกจำนวน 3 ข้อ (ทั้งหมด หลอก + จริง มี 4 ข้อ) โดยมาจากการคำนวนที่ผิดพลาด\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tx2hz5ey4sBE",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6166ac896b334a3ea7df57750d0c0499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7589a68e46441ce9bcbe35103c060ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021114e9142445538145dd0bdc50eec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3128c950ec04d0395899bc16ebc40a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b646d0dd884b3f997254edcca0f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd17940be03247da87bd7c0417bc3fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load tokenizer & model\n",
    "# trust_remote_code may be needed for some chat templates\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754945179366,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "K_PzfnC1tVeN"
   },
   "outputs": [],
   "source": [
    "def call_llm(user_prompt, system_prompt, temperature=0.8, top_p=0.95, max_new_tokens=9216):\n",
    "    messages = []\n",
    "    \n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Use chat template if available\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # Fallback: simple concatenation\n",
    "        full_prompt = (system_prompt + \"\\n\\n\" if system_prompt else \"\") + user_prompt\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1754946094390,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "2rEkmUmmvpQf"
   },
   "outputs": [],
   "source": [
    "def extract_think_and_content(output_str):\n",
    "    think, content = \"\", \"\"\n",
    "    if \"<think>\" in output_str and \"</think>\" in output_str:\n",
    "        think = output_str.split(\"<think>\")[1].split(\"</think>\")[0].replace(\"\\n\", \" \").strip()\n",
    "    if \"<questions>\" in output_str:\n",
    "        content = output_str.split(\"<questions>\", 1)[1]\n",
    "        content = \"<questions>\" + content\n",
    "        content = content.replace(\"\\n\", \" \").strip()\n",
    "    return think, content\n",
    "\n",
    "\n",
    "def save_result(think, content, params, result_meta=None, outdir=\"results\"):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    run_id = int(time.time())\n",
    "    record = {\n",
    "        \"topic\": params[0],\n",
    "        \"grade\": params[1],\n",
    "        \"qtype\": params[2],\n",
    "        \"level\": params[3],\n",
    "        \"bloom\": params[4],\n",
    "        \"think\": think,\n",
    "        \"content\": content,\n",
    "    }\n",
    "    with open(f\"{outdir}/result_{run_id}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754946094752,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "Y2dswVG-47Y-"
   },
   "outputs": [],
   "source": [
    "# full_prompt = f\"{SYSTEM_PROMPT}\\n\\nUser: {USER_PROMPT}\\nAssistant:\"\n",
    "\n",
    "# sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=8192)\n",
    "# output = llm.generate(full_prompt, sampling_params)\n",
    "\n",
    "# for output_item in output:\n",
    "#     prompt = output_item.prompt\n",
    "#     generated_text = output_item.outputs[0].text\n",
    "#     wrapped_text = textwrap.fill(generated_text, width=80)\n",
    "#     print(f\"{wrapped_text}\")\n",
    "\n",
    "# result = call_llm(create_user_prompt(*all_combinations[0]))\n",
    "\n",
    "def _first_text_from_result(result: dict) -> str:\n",
    "    try:\n",
    "        out = result.get(\"output\", [])\n",
    "        if not out:\n",
    "            return \"\"\n",
    "        choices = out[0].get(\"choices\", [])\n",
    "        if choices:\n",
    "            msg = choices[0]\n",
    "            tokens = msg.get(\"tokens\")\n",
    "            if isinstance(tokens, list):\n",
    "                return \"\".join(tokens)\n",
    "            content = msg.get(\"message\", {}).get(\"content\")\n",
    "            if isinstance(content, str):\n",
    "                return content\n",
    "        return json.dumps(out[0], ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_one_combination(params, outdir=\"results\"):\n",
    "    topic, grade, qtype, level, bloom = params\n",
    "    user_prompt = create_user_prompt(topic, grade, qtype, level, bloom)\n",
    "    result_text = call_llm(user_prompt, system_prompt=SYSTEM_PROMPT)\n",
    "    if not result_text:\n",
    "        return None\n",
    "\n",
    "    think, content = extract_think_and_content(result_text)\n",
    "\n",
    "    # Save as JSON per sample\n",
    "    meta = None\n",
    "    save_result(think, content, params=params, result_meta=meta, outdir=outdir)\n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"grade\": grade,\n",
    "        \"qtype\": qtype,\n",
    "        \"level\": level,\n",
    "        \"bloom\": bloom,\n",
    "        \"think\": think,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HjiP_JLI0GF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: {'topic': 'พีชคณิต', 'grade': 'ม.4', 'qtype': 'multiple_choice', 'level': 'ง่าย', 'bloom': ['จำ'], 'think': '', 'content': '<questions>   <question>     <text>สูตรในการแก้สมการเชิงเส้น $ ax + b = 0 $ คือข้อใด</text>     <type>multiple_choice</t...'}\n"
     ]
    }
   ],
   "source": [
    "# ทดสอบระบบ (full system test 1 รอบ)\n",
    "try:\n",
    "    _ = all_combinations[0]\n",
    "except NameError:\n",
    "    # ensure combinations are built if this cell runs before\n",
    "    bloom_combinations = [[b] for b in BLOOM_LEVELS] + [list(pair) for pair in combinations(BLOOM_LEVELS, 2)]\n",
    "    all_combinations = list(product(TOPICS, GRADE_LEVELS, QUESTION_TYPES, LEVELS, bloom_combinations))\n",
    "\n",
    "test_result = process_one_combination(all_combinations[0], outdir=\"test_results\")\n",
    "print(\"Test result:\", {k: (v[:120]+\"...\") if isinstance(v, str) and len(v) > 120 else v for k, v in (test_result or {}).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tPQXWhy_8FcD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985d2367b6fc4492a9a6a6312c725ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 8.06 MiB is free. Process 5063 has 9.73 GiB memory in use. Process 9649 has 12.48 GiB memory in use. Including non-PyTorch memory, this process has 9.13 GiB memory in use. Of the allocated memory 8.40 GiB is allocated by PyTorch, and 141.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_5063/3856942845.py\", line 36, in process_one_combination\n  File \"/tmp/ipykernel_5063/3563407403.py\", line 19, in call_llm\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2634, in generate\n    result = self._sample(\n             ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3618, in _sample\n    outputs = model_forward(**model_inputs, return_dict=True)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 959, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 481, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n                                       ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 1083, in wrapper\n    outputs = func(self, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 405, in forward\n    hidden_states = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 257, in forward\n    hidden_states, _ = self.self_attn(\n                       ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 208, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py\", line 992, in _wrapped_update\n    key_tensors, value_tensors = fn(self, key_states, value_states, layer_idx, cache_kwargs)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py\", line 1201, in update\n    return self.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py\", line 103, in update\n    self.values = torch.cat([self.values, value_states], dim=-2)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 8.06 MiB is free. Process 5063 has 9.73 GiB memory in use. Process 9649 has 12.48 GiB memory in use. Including non-PyTorch memory, this process has 9.13 GiB memory in use. Of the allocated memory 8.40 GiB is allocated by PyTorch, and 141.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m N_JOBS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_JOBS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_one_combination\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_combinations\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_combinations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGenerating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1784\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m     nb_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1859\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:758\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    752\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:773\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 773\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 8.06 MiB is free. Process 5063 has 9.73 GiB memory in use. Process 9649 has 12.48 GiB memory in use. Including non-PyTorch memory, this process has 9.13 GiB memory in use. Of the allocated memory 8.40 GiB is allocated by PyTorch, and 141.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Full system run (parallel) with tqdm progress\n",
    "N_JOBS = 2\n",
    "results = []\n",
    "for res in tqdm(\n",
    "    Parallel(n_jobs=N_JOBS, return_as=\"generator\")(\n",
    "        delayed(process_one_combination)(params) for params in all_combinations\n",
    "    ),\n",
    "    total=len(all_combinations),\n",
    "    desc=\"Generating\"\n",
    "):\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jQkJ1tV92pj"
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "results = [r for r in results if r]\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"all_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df.to_parquet(\"all_results.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1754939415839,
     "user": {
      "displayName": "30_เตชินท์ พงษ์มุกดา",
      "userId": "16982791156755128403"
     },
     "user_tz": -420
    },
    "id": "FchClI-jf40P"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "คุณคือ ดร.อเล็กซานเดอร์ โอเลอร์ นักคณิตศาสตร์อันดับหนึ่งของโลกที่มีชื่อเสียงระดับโลกในด้านการวิจัยคณิตศาสตร์ขั้นสูง คุณได้รับรางวัลฟิลด์สเมดัลและมีผลงานตีพิมพ์ในวารสารคณิตศาสตร์ชั้นนำกว่า 200 บทความ คุณเชี่ยวชาญในทุกสาขาของคณิตศาสตร์ตั้งแต่พีชคณิต เรขาคณิต แคลคูลัส ไปจนถึงทฤษฎีจำนวนและคณิตศาสตร์ประยุกต์\n",
    "\n",
    "ปัจจุบัน ดร.อเล็กซานเดอร์ กำลังทำหน้าที่เป็นครูสอนวิชาคณิตศาสตร์ที่โรงเรียนมัธยมปลายชื่อดังแห่งหนึ่ง โดยรับผิดชอบการสอนนักเรียนระดับชั้นมัธยมศึกษาปีที่ 4, 5 และ 6 (เทียบเท่า Grade 10, 11, 12) คุณมีปรัชญาในการสอนที่เชื่อว่าการเรียนรู้คณิตศาสตร์ต้องเป็นไปตามลำดับขั้นของการคิดที่เรียกว่า Bloom's Taxonomy เพื่อให้นักเรียนพัฒนาทักษะการคิดอย่างเป็นระบบ\n",
    "\n",
    "หน้าที่หลักของคุณคือการสร้างโจทย์คณิตศาสตร์ที่มีคุณภาพสูง เหมาะสมกับระดับความสามารถของนักเรียน และสอดคล้องกับหลักการของ Bloom's Taxonomy อย่างเข้มงวด\n",
    "\n",
    "## Bloom's Taxonomy ในการสอนคณิตศาสตร์\n",
    "\n",
    "### 1. จำ (Remember)\n",
    "**ความหมาย**: การจำและการเรียกคืนข้อมูล สูตร หรือขั้นตอนพื้นฐานที่ได้เรียนมาแล้ว\n",
    "**คำสำคัญ**: จำได้, ระบุ, รายการ, ตั้งชื่อ, เลือก\n",
    "\n",
    "### 2. เข้าใจ (Understand)\n",
    "**ความหมาย**: การเข้าใจความหมาย แนวคิด และสามารถอธิบายด้วยคำพูดของตนเองได้\n",
    "**คำสำคัญ**: อธิบาย, แปล, สรุป, เปรียบเทียบ, แสดงให้เห็น\n",
    "\n",
    "### 3. นำไปใช้ (Apply)\n",
    "**ความหมาย**: การนำความรู้ สูตร หรือขั้นตอนที่เรียนมาไปใช้แก้ปัญหาในสถานการณ์ใหม่\n",
    "**คำสำคัญ**: คำนวณ, แก้, ใช้, แสดง, ดำเนินการ\n",
    "\n",
    "### 4. วิเคราะห์ (Analyze)\n",
    "**ความหมาย**: การแยกแยะองค์ประกอบ วิเคราะห์ความสัมพันธ์ และเข้าใจโครงสร้างของปัญหา\n",
    "**คำสำคัญ**: แยกแยะ, เปรียบเทียบ, ตรวจสอบ, ทดสอบ, วิเคราะห์\n",
    "\n",
    "### 5. ประเมิน (Evaluate)\n",
    "**ความหมาย**: การตัดสิน ประเมินค่า ให้เหตุผล และแสดงความคิดเห็นโดยใช้เกณฑ์ที่กำหนด\n",
    "**คำสำคัญ**: ตัดสิน, ประเมิน, วิจารณ์, แสดงความคิดเห็น, ให้เหตุผล\n",
    "\n",
    "### 6. สร้างสรรค์ (Create)\n",
    "**ความหมาย**: การสร้างสรรค์สิ่งใหม่ ออกแบบ วางแผน หรือสร้างโจทย์ปัญหาขึ้นมาเอง\n",
    "**คำสำคัญ**: สร้าง, ออกแบบ, วางแผน, เสนอ, พัฒนา\n",
    "\n",
    "## ตัวอย่างโจทย์ตาม Bloom's Taxonomy\n",
    "\n",
    "### จำ (Remember)\n",
    "โจทย์: สูตรหาพื้นที่วงกลมคือข้อใด\n",
    "A) $ A = \\pi r^2 $  B) $ A = 2\\pi r $  C) $ A = \\pi d $  D) $ A = \\frac{1}{2}\\pi r^2 $\n",
    "\n",
    "### เข้าใจ (Understand)\n",
    "โจทย์: อธิบายความหมายของ $ \\frac{dy}{dx} = 3x^2 $ ในทางเรขาคณิต\n",
    "\n",
    "### นำไปใช้ (Apply)\n",
    "โจทย์: จงหาค่า $ x $ จากสมการ $ 2x + 5 = 13 $\n",
    "\n",
    "### วิเคราะห์ (Analyze)\n",
    "โจทย์: เปรียบเทียบพฤติกรรมของฟังก์ชัน $ f(x) = x^2 $ และ $ g(x) = x^3 $ เมื่อ $ x > 1 $\n",
    "\n",
    "### ประเมิน (Evaluate)\n",
    "โจทย์: นักเรียนคนหนึ่งอ้างว่า $ \\sqrt{a + b} = \\sqrt{a} + \\sqrt{b} $ เสมอ จงประเมินว่าข้อความนี้ถูกหรือผิด พร้อมให้เหตุผล\n",
    "\n",
    "### สร้างสรรค์ (Create)\n",
    "โจทย์: ออกแบบฟังก์ชันกำลังสองที่มีจุดยอดอยู่ที่ $ (2, -3) $ และผ่านจุด $ (0, 1) $\n",
    "\n",
    "## ตัวอย่างโจทย์แบบผสม 5 โจทย์\n",
    "\n",
    "### โจทย์ที่ 1 (เข้าใจ + นำไปใช้)\n",
    "หากฟังก์ชัน $ f(x) = 2x - 3 $ จงหาค่า $ f(5) $ และอธิบายความหมายของผลลัพธ์\n",
    "\n",
    "### โจทย์ที่ 2 (วิเคราะห์ + ประเมิน)\n",
    "เปรียบเทียบวิธีแก้สมการ $ x^2 - 5x + 6 = 0 $ ด้วยการแยกตัวประกอบและสูตรกำลังสอง แล้วประเมินว่าวิธีใดมีประสิทธิภาพมากกว่า\n",
    "\n",
    "### โจทย์ที่ 3 (นำไปใช้ + วิเคราะห์)\n",
    "ร้านค้าแห่งหนึ่งขายสินค้าในราคา $ 100x - x^2 $ บาท เมื่อขาย $ x $ ชิ้น จงหาจำนวนสินค้าที่ขายได้เงินสูงสุด\n",
    "\n",
    "### โจทย์ที่ 4 (เข้าใจ + สร้างสรรค์)\n",
    "จากกราฟ $ y = \\sin x $ จงสร้างฟังก์ชันใหม่ที่มีแอมพลิจูดเป็น 3 เท่า และอธิบายการเปลี่ยนแปลง\n",
    "\n",
    "### โจทย์ที่ 5 (วิเคราะห์ + ประเมิน + สร้างสรรค์)\n",
    "นักเรียนสองคนแก้โจทย์หาค่าสูงสุดของ $ f(x) = -x^2 + 4x + 1 $ ได้คำตอบต่างกัน คนหนึ่งได้ $ x = 2 $ อีกคนได้ $ x = -2 $ จงวิเคราะห์ว่าใครถูก ประเมินข้อผิดพลาด และสร้างวิธีตรวจสอบคำตอบ\n",
    "\n",
    "---\n",
    "\n",
    "## คำสั่งการทำงาน\n",
    "\n",
    "เมื่อได้รับโจทย์ให้สร้างข้อสอบคณิตศาสตร์ ให้ทำตามขั้นตอนดังนี้:\n",
    "\n",
    "### ขั้นตอนที่ 1: การคิดและวางแผน\n",
    "ในแท็ก `<think></think>` ให้ทำการ:\n",
    "1. **คิดโจทย์เป็นภาษาอังกฤษก่อน** - เพื่อให้ได้แนวคิดที่ชัดเจน\n",
    "2. **ทดลองหาคำตอบ** - ตรวจสอบว่าโจทย์สามารถแก้ได้จริง\n",
    "3. **หากแก้ไม่ได้ ให้เปลี่ยนโจทย์** - แล้วทดลองใหม่จนได้โจทย์ที่ดี\n",
    "4. **เมื่อได้โจทย์ที่สมบูรณ์แล้ว** - สร้างตัวเลือกคำตอบสำหรับ multiple choice (หากต้องการ)\n",
    "5. **ตรวจสอบความสอดคล้องกับ Bloom's taxonomy** ที่กำหนด\n",
    "\n",
    "### ขั้นตอนที่ 2: การเขียนผลลัพธ์\n",
    "หลังจาก `<think></think>` แล้ว ให้เขียนโจทย์ในรูปแบบ XML ตามตัวอย่างนี้:\n",
    "\n",
    "```xml\n",
    "<questions>\n",
    "  <question>\n",
    "    <text>โจทย์คณิตศาสตร์ที่มี KaTeX formatting เช่น $ 4x + 3 = 2x + 9 $</text>\n",
    "    <type>multiple_choice</type>\n",
    "    <options>\n",
    "      <option>$ ตัวเลือก1 $</option>\n",
    "      <option>$ ตัวเลือก2 $</option>\n",
    "      <option>$ ตัวเลือก3 $</option>\n",
    "      <option>$ ตัวเลือก4 $</option>\n",
    "    </options>\n",
    "    <correct_answer>$ คำตอบที่ถูก $</correct_answer>\n",
    "    <explanation>\n",
    "      ขั้นตอนที่ 1: อธิบายด้วย KaTeX เช่น $ 4x - 2x + 3 = 9 $\n",
    "      ขั้นตอนที่ 2: $ 2x + 3 = 9 $\n",
    "      ขั้นตอนที่ 3: $ 2x = 6 $\n",
    "      ขั้นตอนที่ 4: $ x = 3 $\n",
    "    </explanation>\n",
    "    <score>2</score>\n",
    "    <difficulty>ง่าย</difficulty>\n",
    "    <bloom_levels>\n",
    "      <level>เข้าใจ</level>\n",
    "      <level>วิเคราะห์</level>\n",
    "    </bloom_levels>\n",
    "  </question>\n",
    "\n",
    "  <question>\n",
    "    <text>จงหาค่า $ x $ ที่ทำให้ $ 3x - 7 = 2x + 5 $</text>\n",
    "    <type>short_answer</type>\n",
    "    <correct_answer>$ x = 12 $</correct_answer>\n",
    "    <explanation>\n",
    "      ขั้นตอนที่ 1: นำ $ 2x $ ไปยังข้างซ้าย และ $ 7 $ ไปยังข้างขวา\n",
    "      $ 3x - 2x = 5 + 7 $\n",
    "      ขั้นตอนที่ 2: คำนวณ\n",
    "      $ x = 12 $\n",
    "    </explanation>\n",
    "    <score>2</score>\n",
    "    <difficulty>ปานกลาง</difficulty>\n",
    "    <bloom_levels>\n",
    "      <level>เข้าใจ</level>\n",
    "    </bloom_levels>\n",
    "  </question>\n",
    "</questions>\n",
    "```\n",
    "\n",
    "## หมายเหตุสำคัญ\n",
    "\n",
    "### รูปแบบการเขียน:\n",
    "- **ใช้ KaTeX format** `$ ... $` สำหรับตัวเลข สูตร และนิพจน์คณิตศาสตร์ทั้งหมด\n",
    "- **ใช้คำไทยสำหรับ Bloom's level**: จำ, เข้าใจ, นำไปใช้, วิเคราะห์, ประเมิน, สร้างสรรค์\n",
    "- **สามารถมีหลายระดับ Bloom's**: `<bloom_levels><level>เข้าใจ</level><level>นำไปใช้</level></bloom_levels>`\n",
    "\n",
    "### ระดับความยาก:\n",
    "- **ง่าย**: โจทย์พื้นฐาน 1-2 ขั้นตอน (คะแนน 1-2)\n",
    "- **ปานกลาง**: โจทย์ที่ต้องใช้ความคิด 3-4 ขั้นตอน (คะแนน 3-4)\n",
    "- **ยาก**: โจทย์ซับซ้อน ต้องวิเคราะห์เชิงลึก 5+ ขั้นตอน (คะแนน 5+)\n",
    "\n",
    "### การให้คะแนน:\n",
    "- Score ควรสะท้อนระดับความยากและเวลาที่ใช้ในการแก้ปัญหา\n",
    "- โจทย์ที่ต้องใช้ Bloom's level สูงควรได้คะแนนมากกว่า\n",
    "\n",
    "คุณพร้อมที่จะสร้างโจทย์คณิตศาสตร์คุณภาพสูงตามที่ได้รับมอบหมายแล้วหรือไม่?\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "จงสร้างโจทย์คณิตศาสตร์คุณภาพสูงโดยกำหนดให้\n",
    "1. หัวข้อ: พีชคณิต\n",
    "2. สำหรับนักเรียน: ม.4\n",
    "3. รูปแบบ: multiple_choice\n",
    "4. ความยาก: ยาก\n",
    "5. bloom level: เข้าใจ, นำไปใช้\n",
    "6. จำนวน: 1 ข้อ\n",
    "7. เพิ่มเติม: โจทย์จำเป็นต้องมีคำตอบ และถ้าโจทย์เป็นแบบ multiple choice (ปรนัย) ต้องมีคำตอบหลอกจำนวน 3 ข้อ (ทั้งหมด หลอก + จริง มี 4 ข้อ) โดยมาจากการคำนวนที่ผิดพลาด\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyONmCzWfJpEFCcYJCRgeBRQ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
